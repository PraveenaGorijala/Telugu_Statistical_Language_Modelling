{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "fourGram_with_tokenizer(organised).ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PraveenaGorijala/Telugu_Statistical_Language_Modelling/blob/master/fourGram_with_tokenizer(organised).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ji6WhkHBTwPE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#AIET Team 13\n",
        "#compatible with python 3.7"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5QkhWR8TwPK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#importing packages\n",
        "import re\n",
        "import string\n",
        "import collections\n",
        "import math as m"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kNCGhbUTwPN",
        "colab_type": "code",
        "colab": {},
        "outputId": "3bfd2949-2e45-42ba-e02b-8244d97b7f83"
      },
      "source": [
        "!pip install cltk               #installing cltk\n",
        "from cltk.tokenize.sentence import TokenizeSentence\n",
        "tokenizer = TokenizeSentence('telugu')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: cltk in /home/kirthana/anaconda3/lib/python3.7/site-packages (0.1.109)\n",
            "Requirement already satisfied: python-crfsuite in /home/kirthana/anaconda3/lib/python3.7/site-packages (from cltk) (0.9.6)\n",
            "Requirement already satisfied: pyuca in /home/kirthana/anaconda3/lib/python3.7/site-packages (from cltk) (1.2)\n",
            "Requirement already satisfied: gitpython in /home/kirthana/anaconda3/lib/python3.7/site-packages (from cltk) (2.1.11)\n",
            "Requirement already satisfied: pyyaml in /home/kirthana/anaconda3/lib/python3.7/site-packages (from cltk) (5.1)\n",
            "Requirement already satisfied: regex in /home/kirthana/anaconda3/lib/python3.7/site-packages (from cltk) (2019.6.8)\n",
            "Requirement already satisfied: whoosh in /home/kirthana/anaconda3/lib/python3.7/site-packages (from cltk) (2.7.4)\n",
            "Requirement already satisfied: nltk in /home/kirthana/anaconda3/lib/python3.7/site-packages (from cltk) (3.4)\n",
            "Requirement already satisfied: gitdb2>=2.0.0 in /home/kirthana/anaconda3/lib/python3.7/site-packages (from gitpython->cltk) (2.0.5)\n",
            "Requirement already satisfied: six in /home/kirthana/anaconda3/lib/python3.7/site-packages (from nltk->cltk) (1.12.0)\n",
            "Requirement already satisfied: singledispatch in /home/kirthana/anaconda3/lib/python3.7/site-packages (from nltk->cltk) (3.4.0.3)\n",
            "Requirement already satisfied: smmap2>=2.0.0 in /home/kirthana/anaconda3/lib/python3.7/site-packages (from gitdb2>=2.0.0->gitpython->cltk) (2.0.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLQMk0tWTwPS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#loading and converting the train data into a list of sentences\n",
        "path='/home/kirthana/primary_test_data'      \n",
        "with open(path) as myfile:\n",
        "    lines = [line.split('.') for line in myfile.readlines()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhT0484lTwPV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#loading and converting the test data into a list of sentences\n",
        "testpath='/home/kirthana/test_data'         \n",
        "with open(testpath) as file:\n",
        "    testlines = [line.split('.') for line in file.readlines()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGhriQrpTwPZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#function to tokenize the sentences\n",
        "def getting_sentences(line):               \n",
        "    sentences = []\n",
        "    for i in range(0,len(line)):\n",
        "        for j in range(0,len(line[i])):\n",
        "            sentence = line[i][j]\n",
        "            s = tokenizer.tokenize(sentence)\n",
        "            sentences.append(s)\n",
        "    return sentences\n",
        "train_sentences = getting_sentences(lines)\n",
        "test_sentences = getting_sentences(testlines)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3_JWX8JTwPc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#function removing the html tags,punctuations,numbers,english words\n",
        "def preprocessing_data(str):\n",
        "    u = re.sub('<[^<]+?>','',str)\n",
        "    o = re.sub('\\u200c',' ',u)\n",
        "    table = str.maketrans({key: None for key in string.punctuation})\n",
        "    translated = o.translate(table)\n",
        "    w = re.sub('[a-zA-Z]*','',translated)\n",
        "    d = re.sub(' +',' ',w)\n",
        "    x = re.sub('[0-9]*','',d)\n",
        "    return x\n",
        "\n",
        "#getting a list of sentences that are clean\n",
        "def get_preprocessed_sentences(sentences):           \n",
        "    tokenized_sentences=[]\n",
        "    for i in range(0,len(sentences)):\n",
        "        temp = []\n",
        "        for j in range(0,len(sentences[i])):\n",
        "            final = preprocessing_data(sentences[i][0])\n",
        "            temp.append(final)\n",
        "        tokenized_sentences.append(temp)\n",
        "    return tokenized_sentences\n",
        "\n",
        "train_preprocessed_sentences = get_preprocessed_sentences(train_sentences)\n",
        "test_preprocessed_sentences = get_preprocessed_sentences(test_sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nDBk77PTwPf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# adding start and end delimiter, then spliting\n",
        "def delimiting_spliting(tokenized_sentences):          \n",
        "    tokenized_sentences = [x for x in tokenized_sentences if x != ['\\n'] and x != [' \\n']]\n",
        "    sentences_with_delimiter = []\n",
        "    for i in range(0,len(tokenized_sentences)):\n",
        "        s = tokenized_sentences[i][0]\n",
        "        temp = '<s> '+s+' </s>'                        #adding start and end delimiter\n",
        "        sentences_with_delimiter.append(temp)\n",
        "    split_data =[]\n",
        "    for i in range(0,len(sentences_with_delimiter)):   #spliting the words, and getting the list that contain list of words of every sentence\n",
        "        d = sentences_with_delimiter[i].split()\n",
        "        split_data.append(d)\n",
        "    return split_data\n",
        "split_train_data = delimiting_spliting(train_preprocessed_sentences)\n",
        "split_test_data = delimiting_spliting(test_preprocessed_sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDWHm96fTwPi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#getting a single list of words\n",
        "def getting_words_list(split_data):             \n",
        "    list = [item for sublist in split_data for item in sublist]\n",
        "    return list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkG-mSI8TwPl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words_train = getting_words_list(split_train_data)         #train data words\n",
        "words_test = getting_words_list(split_test_data)           #test data words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22ye8QbcTwPo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "unique_words = len(set(words_train))             # getting number of unique words in train data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "ZlCD-NdATwPr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#geting the dictionary(model) from the train data, where keys are the history words and values are \n",
        "#another dictionary which contains keys as the next word and its values corresponding to the number \n",
        "#of times that words has occurred\n",
        "dic = {}\n",
        "for i in range(0,len(words_train)-3):\n",
        "    t = words_train[i] +'_'+ words_train[i+1] +'_'+ words_train[i+2]\n",
        "    s = dic.get(t)\n",
        "    next = i+3\n",
        "    if s == None:\n",
        "        dic[t] = [words_train[next]]\n",
        "    else:\n",
        "        s.append(words_train[next])\n",
        "        temp = {t:s}\n",
        "        dic.update(temp)\n",
        "        \n",
        "#test_dic is a dictionary where keys are the history words of the test data and values is the list of next words \n",
        "\n",
        "test_dic = {}\n",
        "for i in range(0,len(words_test)-3):\n",
        "    t = words_test[i] +'_'+ words_test[i+1] +'_'+ words_train[i+2]\n",
        "    s = test_dic.get(t)\n",
        "    next = i+3\n",
        "    if s == None:\n",
        "        test_dic[t] = [words_test[next]]\n",
        "    else:\n",
        "        s.append(words_test[next])\n",
        "        temp = {t:s}\n",
        "        test_dic.update(temp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obqb2kYKTwPu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# getting the count of each element in the list\n",
        "\n",
        "def frequency_count(list):\n",
        "    c = collections.Counter(list)\n",
        "    count_dic={}\n",
        "    for i in range(0,len(list)):\n",
        "        count_dic[list[i]] = c[list[i]]\n",
        "    return count_dic\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4oHcdmSTwPw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# updating the dic with the word frequency\n",
        "for word in dic:\n",
        "    s = dic.get(word)\n",
        "    value = frequency_count(s)\n",
        "    temporary = {word:value}\n",
        "    dic.update(temporary)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4foGH3ITwPz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# getting the probability of each predictable word\n",
        "def get_probability(s1,s2,s3,s4):\n",
        "    threeWords = s1+'_'+s2+'_'+s3\n",
        "    nextWord = s4\n",
        "    inner_dic = dic.get(threeWords)\n",
        "    if inner_dic == None:\n",
        "        return 0\n",
        "    else:\n",
        "        numerator = inner_dic.get(nextWord)\n",
        "        if numerator == None:\n",
        "            return 0\n",
        "        else:\n",
        "            denominator = sum(inner_dic.values())\n",
        "            return numerator/denominator\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCC3AZrMTwP1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# getting the add-one probability of each predictable word\n",
        "def get_addOne_probability(s1,s2,s3,s4):\n",
        "    threeWords = s1+'_'+s2+'_'+s3\n",
        "    nextWord = s4\n",
        "    inner_dic = dic.get(threeWords)\n",
        "    if inner_dic == None:\n",
        "        return 0 \n",
        "    else:\n",
        "        numerator = inner_dic.get(nextWord)\n",
        "        if numerator == None:\n",
        "            numerator = 1\n",
        "        else:\n",
        "            numerator = numerator + 1\n",
        "        denominator = sum(inner_dic.values()) + unique_words\n",
        "    return numerator/denominator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwSz3njtTwP5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# getting the add-alpha probability of each predictable word\n",
        "\n",
        "\n",
        "def get_addAlpha_probability(s1,s2,s3,s4):\n",
        "    threeWords = s1+'_'+s2+'_'+s3\n",
        "    nextWord = s4\n",
        "    alpha = 0.1\n",
        "    inner_dic = dic.get(threeWords)\n",
        "    if inner_dic == None:\n",
        "        return 0 \n",
        "    else:\n",
        "        numerator = inner_dic.get(nextWord)\n",
        "        if numerator == None:\n",
        "            numerator = alpha\n",
        "        else:\n",
        "            numerator = numerator + alpha\n",
        "        denominator = sum(inner_dic.values()) + (unique_words*alpha)\n",
        "    return numerator/denominator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xf6YgIBxTwP7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#preparing probability dictionaries with corresponding probabilities\n",
        "prob_dic = {}\n",
        "addOneProb_dic = {}\n",
        "addAlphaProb_dic = {}\n",
        "for i in range(0,len(words_test)-4):\n",
        "    fourgram = words_test[i]+'_'+words_test[i+1]+'_'+words_test[i+2]+'_'+words_test[i+3]\n",
        "    prob = get_probability(words_test[i],words_test[i+1],words_test[i+2],words_test[i+3])\n",
        "    prob_dic[fourgram]=prob\n",
        "    addOneProb = get_addOne_probability(words_test[i],words_test[i+1],words_test[i+2],words_test[i+3])\n",
        "    addOneProb_dic[fourgram] = addOneProb\n",
        "    addAlphaProb = get_addAlpha_probability(words_test[i],words_test[i+1],words_test[i+2],words_test[i+3])\n",
        "    addAlphaProb_dic[fourgram] = addAlphaProb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmQSr0jrTwP_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# computing perplexity Score\n",
        "def perplexityScore(prob_dic):\n",
        "    logProb = 0\n",
        "    num = 0\n",
        "    for word in prob_dic:\n",
        "        p = prob_dic.get(word)\n",
        "        if p == 0:\n",
        "            logProb = logProb\n",
        "        else:\n",
        "            w = m.log(p**-1,10)\n",
        "            logProb = logProb + w\n",
        "            num = num + 1\n",
        "    return (2**(logProb/num))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUvNlyvETwQC",
        "colab_type": "code",
        "colab": {},
        "outputId": "8753820a-02b8-4cd5-a3db-a1eb7ffeb0f5"
      },
      "source": [
        "# perplexity Scores for different cases\n",
        "ans = perplexityScore(prob_dic)\n",
        "print(ans)\n",
        "ans1 = perplexityScore(addOneProb_dic)\n",
        "print(ans1)\n",
        "ans1 = perplexityScore(addAlphaProb_dic)\n",
        "print(ans1)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0994876859760423\n",
            "13.700893568538902\n",
            "11.468977285659376\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TH-UOKtFTwQE",
        "colab_type": "code",
        "colab": {},
        "outputId": "21c1f235-f30a-49f2-8bbe-7928ed5b8a80"
      },
      "source": [
        "#input interface\n",
        "s1 = input(\"Enter the 1st word \" )\n",
        "s2 = input(\"Enter the 2nd word \" )\n",
        "s3 = input(\"Enter the 3rd word \")\n",
        "input_word = s1 + s2 + s3\n",
        "print(input_word)\n",
        "def getting_4th(input_word):\n",
        "    s = dic.get(input_word)\n",
        "    if s == None:\n",
        "        print(\"not found\")\n",
        "        return\n",
        "    else:          #గుంటూరు చకిమీ\n",
        "        maximum = max(s, key=s.get)\n",
        "        return maximum\n",
        "getting_4th(input_word)\n",
        "        "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter the 1st word గుంటూరు\n",
            "Enter the 2nd word జిల్లా\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbF6cmI-TwQG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}