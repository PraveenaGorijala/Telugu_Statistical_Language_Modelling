{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "triGram_with_tokenizer(organised).ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PraveenaGorijala/Telugu_Statistical_Language_Modelling/blob/master/triGram_with_tokenizer(organised).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rpZhAYgTiv8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#AIET Team 13\n",
        "# compatible with python3.7"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0KTDQ6yTiwB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#importing packages\n",
        "import re\n",
        "import string\n",
        "import collections\n",
        "import math as m"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "somrU6CJTiwF",
        "colab_type": "code",
        "colab": {},
        "outputId": "acad1939-a613-4aef-f35d-3bbba5991b32"
      },
      "source": [
        "!pip install cltk                  #intalling cltk\n",
        "from cltk.tokenize.sentence import TokenizeSentence\n",
        "tokenizer = TokenizeSentence('telugu')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: cltk in /home/kirthana/anaconda3/lib/python3.7/site-packages (0.1.109)\n",
            "Requirement already satisfied: python-crfsuite in /home/kirthana/anaconda3/lib/python3.7/site-packages (from cltk) (0.9.6)\n",
            "Requirement already satisfied: regex in /home/kirthana/anaconda3/lib/python3.7/site-packages (from cltk) (2019.6.8)\n",
            "Requirement already satisfied: whoosh in /home/kirthana/anaconda3/lib/python3.7/site-packages (from cltk) (2.7.4)\n",
            "Requirement already satisfied: pyuca in /home/kirthana/anaconda3/lib/python3.7/site-packages (from cltk) (1.2)\n",
            "Requirement already satisfied: nltk in /home/kirthana/anaconda3/lib/python3.7/site-packages (from cltk) (3.4)\n",
            "Requirement already satisfied: gitpython in /home/kirthana/anaconda3/lib/python3.7/site-packages (from cltk) (2.1.11)\n",
            "Requirement already satisfied: pyyaml in /home/kirthana/anaconda3/lib/python3.7/site-packages (from cltk) (5.1)\n",
            "Requirement already satisfied: six in /home/kirthana/anaconda3/lib/python3.7/site-packages (from nltk->cltk) (1.12.0)\n",
            "Requirement already satisfied: singledispatch in /home/kirthana/anaconda3/lib/python3.7/site-packages (from nltk->cltk) (3.4.0.3)\n",
            "Requirement already satisfied: gitdb2>=2.0.0 in /home/kirthana/anaconda3/lib/python3.7/site-packages (from gitpython->cltk) (2.0.5)\n",
            "Requirement already satisfied: smmap2>=2.0.0 in /home/kirthana/anaconda3/lib/python3.7/site-packages (from gitdb2>=2.0.0->gitpython->cltk) (2.0.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZz6tmWfTiwK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#loading and converting the train data into a list of sentences\n",
        "path='/home/kirthana/primary_test_data'            \n",
        "with open(path) as myfile:\n",
        "    lines = [line.split('.') for line in myfile.readlines()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqLWp8SPTiwP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#loading and converting the test data into a list of sentences\n",
        "testpath='/home/kirthana/test_data'                \n",
        "with open(testpath) as file:\n",
        "    testlines = [line.split('.') for line in file.readlines()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SH0fZhsBTiwS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#function to tokenize the sentences\n",
        "def getting_sentences(line):                       \n",
        "    sentences = []\n",
        "    for i in range(0,len(line)):\n",
        "        for j in range(0,len(line[i])):\n",
        "            sentence = line[i][j]\n",
        "            s = tokenizer.tokenize(sentence)\n",
        "            sentences.append(s)\n",
        "    return sentences\n",
        "train_sentences = getting_sentences(lines)\n",
        "test_sentences = getting_sentences(testlines)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTxmhY-bTiwV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#function removing the html tags,punctuations,numbers,english words\n",
        "def preprocessing_data(str):                    \n",
        "    u = re.sub('<[^<]+?>','',str)\n",
        "    o = re.sub('\\u200c',' ',u)\n",
        "    table = str.maketrans({key: None for key in string.punctuation})\n",
        "    translated = o.translate(table)\n",
        "    w = re.sub('[a-zA-Z]*','',translated)\n",
        "    d = re.sub(' +',' ',w)\n",
        "    x = re.sub('[0-9]*','',d)\n",
        "    return x\n",
        "\n",
        "#getting a list of sentences that are clean\n",
        "def get_preprocessed_sentences(sentences):    \n",
        "    tokenized_sentences=[]\n",
        "    for i in range(0,len(sentences)):\n",
        "        temp = []\n",
        "        for j in range(0,len(sentences[i])):\n",
        "            final = preprocessing_data(sentences[i][0])\n",
        "            temp.append(final)\n",
        "        tokenized_sentences.append(temp)\n",
        "    return tokenized_sentences\n",
        "\n",
        "train_preprocessed_sentences = get_preprocessed_sentences(train_sentences)\n",
        "test_preprocessed_sentences = get_preprocessed_sentences(test_sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obSKV_AOTiwZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# adding start and end delimiter, then spliting\n",
        "def delimiting_spliting(tokenized_sentences):        \n",
        "    tokenized_sentences = [x for x in tokenized_sentences if x != ['\\n'] and x != [' \\n']]\n",
        "    sentences_with_delimiter = []\n",
        "    for i in range(0,len(tokenized_sentences)):\n",
        "        s = tokenized_sentences[i][0]\n",
        "        temp = '<s> '+s+' </s>'                     #adding start and end delimiter\n",
        "        sentences_with_delimiter.append(temp)\n",
        "    split_data =[]\n",
        "    for i in range(0,len(sentences_with_delimiter)):    #spliting the words, and getting the list that contain list of words of every sentence\n",
        "        d = sentences_with_delimiter[i].split()\n",
        "        split_data.append(d)\n",
        "    return split_data\n",
        "split_train_data = delimiting_spliting(train_preprocessed_sentences)\n",
        "split_test_data = delimiting_spliting(test_preprocessed_sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QkEKX76Tiwc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#getting a single list of words\n",
        "def getting_words_list(split_data):      \n",
        "    list = [item for sublist in split_data for item in sublist]\n",
        "    return list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6QL0kBSTiwf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words_train = getting_words_list(split_train_data)      #train data words\n",
        "words_test = getting_words_list(split_test_data)        #test data words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pelFkqGITiwi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "unique_words = len(set(words_train))       # getting number of unique words in train data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "o1LgI0tzTiwn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#geting the dictionary(model) from the train data, where keys are the history words and values are \n",
        "#another dictionary which contains keys as the next word and its values corresponding to the number \n",
        "#of times that words has occurred\n",
        "dic = {}\n",
        "for i in range(0,len(words_train)-2):\n",
        "    t = words_train[i] +'_'+ words_train[i+1]\n",
        "    s = dic.get(t)\n",
        "    next = i+2\n",
        "    if s == None:\n",
        "        dic[t] = [words_train[next]]\n",
        "    else:\n",
        "        s.append(words_train[next])\n",
        "        temp = {t:s}\n",
        "        dic.update(temp)\n",
        "#test_dic is a dictionary where keys are the history words of the test data and values is the list of next words \n",
        "\n",
        "test_dic = {}\n",
        "for i in range(0,len(words_test)-2):\n",
        "    t = words_test[i] +'_'+ words_test[i+1]\n",
        "    s = test_dic.get(t)\n",
        "    next = i+2\n",
        "    if s == None:\n",
        "        test_dic[t] = [words_test[next]]\n",
        "    else:\n",
        "        s.append(words_test[next])\n",
        "        temp = {t:s}\n",
        "        test_dic.update(temp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blsveq2HTiwp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# getting the count of each element in the list\n",
        "def frequency_count(list):\n",
        "    c = collections.Counter(list)\n",
        "    count_dic={}\n",
        "    for i in range(0,len(list)):\n",
        "        count_dic[list[i]] = c[list[i]]\n",
        "    return count_dic\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwoFGlMWTiws",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# updating the dic with the word frequency\n",
        "for word in dic:\n",
        "    s = dic.get(word)\n",
        "    value = frequency_count(s)\n",
        "    temporary = {word:value}\n",
        "    dic.update(temporary)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EweCpAmGTiwv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# getting the probability of each predictable word\n",
        "def get_probability(s1,s2,s3):\n",
        "    twoWords = s1+'_'+s2\n",
        "    nextWord = s3\n",
        "    inner_dic = dic.get(twoWords)\n",
        "    if inner_dic == None:\n",
        "        return 0\n",
        "    else:\n",
        "        numerator = inner_dic.get(nextWord)\n",
        "        if numerator == None:\n",
        "            return 0\n",
        "        else:\n",
        "            denominator = sum(inner_dic.values())\n",
        "            return numerator/denominator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u44L81NSTiwz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# getting the add-one probability of each predictable word\n",
        "def get_addOne_probability(s1,s2,s3):\n",
        "    twoWords = s1+'_'+s2\n",
        "    nextWord = s3\n",
        "    inner_dic = dic.get(twoWords)\n",
        "    if inner_dic == None:\n",
        "        return 0 \n",
        "    else:\n",
        "        numerator = inner_dic.get(nextWord)\n",
        "        if numerator == None:\n",
        "            numerator = 1\n",
        "        else:\n",
        "            numerator = numerator + 1\n",
        "        denominator = sum(inner_dic.values()) + unique_words\n",
        "    return numerator/denominator\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWqovkAaTiw2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# getting the add-alpha probability of each predictable word\n",
        "def get_addAlpha_probability(s1,s2,s3):\n",
        "    twoWords = s1+'_'+s2\n",
        "    nextWord = s3\n",
        "    alpha = 0.1\n",
        "    inner_dic = dic.get(twoWords)\n",
        "    if inner_dic == None:\n",
        "        return 0 #alpha/(alpha*unique_words)\n",
        "    else:\n",
        "        numerator = inner_dic.get(nextWord)\n",
        "        if numerator == None:\n",
        "            numerator = alpha\n",
        "        else:\n",
        "            numerator = numerator + alpha\n",
        "        denominator = sum(inner_dic.values()) + (unique_words*alpha)\n",
        "    return numerator/denominator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEGDf61OTiw6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#preparing probability dictionaries with corresponding probabilities\n",
        "prob_dic = {}\n",
        "addOneProb_dic = {}\n",
        "addAlphaProb_dic = {}\n",
        "for i in range(0,len(words_test)-3):\n",
        "    trigram = words_test[i]+'_'+words_test[i+1]+'_'+words_test[i+2]\n",
        "    prob = get_probability(words_test[i],words_test[i+1],words_test[i+2])\n",
        "    prob_dic[trigram]=prob\n",
        "    addOneProb = get_addOne_probability(words_test[i],words_test[i+1],words_test[i+2])\n",
        "    addOneProb_dic[trigram] = addOneProb\n",
        "    addAlphaProb = get_addAlpha_probability(words_test[i],words_test[i+1],words_test[i+2])\n",
        "    addAlphaProb_dic[trigram] = addAlphaProb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEmldgURTiw-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# computing perplexity Score\n",
        "def perplexityScore(prob_dic):\n",
        "    logProb = 0\n",
        "    num = 0\n",
        "    for word in prob_dic:\n",
        "        p = prob_dic.get(word)\n",
        "        if p == 0:\n",
        "            logProb = logProb\n",
        "        else:\n",
        "            w = m.log(p**-1,10)\n",
        "            logProb = logProb + w\n",
        "            num = num + 1\n",
        "    return (2**(logProb/num))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99hLQEPgTixA",
        "colab_type": "code",
        "colab": {},
        "outputId": "181bf546-50cd-492f-f08e-e705670aa897"
      },
      "source": [
        "# perplexity Scores for different cases\n",
        "ans = perplexityScore(prob_dic)\n",
        "print(ans)\n",
        "ans1 = perplexityScore(addOneProb_dic)\n",
        "print(ans1)\n",
        "ans1 = perplexityScore(addAlphaProb_dic)\n",
        "print(ans1)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.8164982084209653\n",
            "13.430508366547492\n",
            "12.345086404944007\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clVndOzMTixD",
        "colab_type": "code",
        "colab": {},
        "outputId": "19c1b5b8-f50e-4a0b-a9b1-92791ba839c2"
      },
      "source": [
        "#input interface\n",
        "s1 = input(\"Enter the 1st word \" )\n",
        "s2 = input(\"Enter the 2nd word \" )\n",
        "input_word = s1 +'_'+ s2\n",
        "def getting_3rd(input_word):\n",
        "    s = dic.get(input_word)\n",
        "    if s == None:\n",
        "        print(\"not found\")\n",
        "        return\n",
        "    else:\n",
        "        maximum = max(s, key=s.get)\n",
        "        return maximum\n",
        "getting_3rd(input_word)\n",
        "#printing a sequence of words\n",
        "i = 1\n",
        "third = getting_3rd(input_word)\n",
        "for i in range(1,20):\n",
        "        print(third)    \n",
        "        s1 = s2         \n",
        "        s2 = third\n",
        "        input_word = s1 +'_'+ s2\n",
        "        third = getting_3rd(input_word)\n",
        "       \n",
        "        "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter the 1st word వాయవ్య\n",
            "Enter the 2nd word అంచున\n",
            "గల\n",
            "కొండలు\n",
            "మల్లవరం\n",
            "దగ్గర\n",
            "కృష్ణానదిలో\n",
            "కలిసేవరకు\n",
            "ఉన్నాయి\n",
            "</s>\n",
            "<s>\n",
            "ఈ\n",
            "రోజుల్లో\n",
            "కంప్యూటర్\n",
            "అంటే\n",
            "యంత్రమే\n",
            "మనిషి\n",
            "కాదు\n",
            "</s>\n",
            "<s>\n",
            "ఈ\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}