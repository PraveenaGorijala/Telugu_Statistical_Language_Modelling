{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "biGram_with_tokenizer(organised).ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PraveenaGorijala/Telugu_Statistical_Language_Modelling/blob/master/biGram_with_tokenizer(organised).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gixemOKtSfLD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#AIET Team 13\n",
        "# compatible  with python 3.7"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8y3MwrKMSfLI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#importing  packages\n",
        "import re\n",
        "import string\n",
        "import collections\n",
        "import math as m"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgvDmgnvSfLN",
        "colab_type": "code",
        "colab": {},
        "outputId": "66121f36-25b7-4692-aceb-a22cd2b4d914"
      },
      "source": [
        "!pip install cltk     #installing cltk\n",
        "from cltk.tokenize.sentence import TokenizeSentence\n",
        "tokenizer = TokenizeSentence('telugu')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: cltk in /home/kirthana/anaconda3/lib/python3.7/site-packages (0.1.109)\n",
            "Requirement already satisfied: nltk in /home/kirthana/anaconda3/lib/python3.7/site-packages (from cltk) (3.4)\n",
            "Requirement already satisfied: pyuca in /home/kirthana/anaconda3/lib/python3.7/site-packages (from cltk) (1.2)\n",
            "Requirement already satisfied: pyyaml in /home/kirthana/anaconda3/lib/python3.7/site-packages (from cltk) (5.1)\n",
            "Requirement already satisfied: whoosh in /home/kirthana/anaconda3/lib/python3.7/site-packages (from cltk) (2.7.4)\n",
            "Requirement already satisfied: regex in /home/kirthana/anaconda3/lib/python3.7/site-packages (from cltk) (2019.6.8)\n",
            "Requirement already satisfied: gitpython in /home/kirthana/anaconda3/lib/python3.7/site-packages (from cltk) (2.1.11)\n",
            "Requirement already satisfied: python-crfsuite in /home/kirthana/anaconda3/lib/python3.7/site-packages (from cltk) (0.9.6)\n",
            "Requirement already satisfied: six in /home/kirthana/anaconda3/lib/python3.7/site-packages (from nltk->cltk) (1.12.0)\n",
            "Requirement already satisfied: singledispatch in /home/kirthana/anaconda3/lib/python3.7/site-packages (from nltk->cltk) (3.4.0.3)\n",
            "Requirement already satisfied: gitdb2>=2.0.0 in /home/kirthana/anaconda3/lib/python3.7/site-packages (from gitpython->cltk) (2.0.5)\n",
            "Requirement already satisfied: smmap2>=2.0.0 in /home/kirthana/anaconda3/lib/python3.7/site-packages (from gitdb2>=2.0.0->gitpython->cltk) (2.0.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JN7eiHkWSfLQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#loading and converting the train data into a list of sentences\n",
        "path='/home/kirthana/primary_test_data'    \n",
        "with open(path) as myfile:\n",
        "    lines = [line.split('.') for line in myfile.readlines()]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdqaIONBSfLT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#loading and converting the test data into a list of sentences\n",
        "testpath='/home/kirthana/test_data'        \n",
        "with open(testpath) as file:\n",
        "    testlines = [line.split('.') for line in file.readlines()]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEAs2ktBSfLW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#function to tokenize the sentences\n",
        "def getting_sentences(line):               \n",
        "    sentences = []\n",
        "    for i in range(0,len(line)):\n",
        "        for j in range(0,len(line[i])):\n",
        "            sentence = line[i][j]\n",
        "            s = tokenizer.tokenize(sentence)\n",
        "            sentences.append(s)\n",
        "    return sentences\n",
        "train_sentences = getting_sentences(lines)\n",
        "test_sentences = getting_sentences(testlines)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stDC9up_SfLZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#function removing the html tags,punctuations,numbers,english words\n",
        "def preprocessing_data(str):           \n",
        "    u = re.sub('<[^<]+?>','',str)\n",
        "    o = re.sub('\\u200c',' ',u)\n",
        "    table = str.maketrans({key: None for key in string.punctuation})\n",
        "    translated = o.translate(table)\n",
        "    w = re.sub('[a-zA-Z]*','',translated)\n",
        "    d = re.sub(' +',' ',w)\n",
        "    x = re.sub('[0-9]*','',d)\n",
        "    return x\n",
        "\n",
        "#getting a list of sentences that are clean\n",
        "def get_preprocessed_sentences(sentences):    \n",
        "    tokenized_sentences=[]\n",
        "    for i in range(0,len(sentences)):\n",
        "        temp = []\n",
        "        for j in range(0,len(sentences[i])):\n",
        "            final = preprocessing_data(sentences[i][0])\n",
        "            temp.append(final)\n",
        "        tokenized_sentences.append(temp)\n",
        "    return tokenized_sentences\n",
        "\n",
        "train_preprocessed_sentences = get_preprocessed_sentences(train_sentences)\n",
        "test_preprocessed_sentences = get_preprocessed_sentences(test_sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUI4z0k_SfLc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# adding start and end delimiter, then splitting\n",
        "def delimiting_spliting(tokenized_sentences):   \n",
        "    tokenized_sentences = [x for x in tokenized_sentences if x != ['\\n'] and x != [' \\n']]\n",
        "    sentences_with_delimiter = []\n",
        "    for i in range(0,len(tokenized_sentences)):\n",
        "        s = tokenized_sentences[i][0]\n",
        "        temp = '<s> '+s+' </s>'                #adding start and end delimiter\n",
        "        sentences_with_delimiter.append(temp)\n",
        "    split_data =[]\n",
        "    for i in range(0,len(sentences_with_delimiter)):\n",
        "        d = sentences_with_delimiter[i].split()\n",
        "        split_data.append(d)                #spliting the words, and getting the list that contain list of words of every sentence\n",
        "    return split_data\n",
        "split_train_data = delimiting_spliting(train_preprocessed_sentences)\n",
        "split_test_data = delimiting_spliting(test_preprocessed_sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXJQXA9LSfLf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#getting a single list of words\n",
        "def getting_words_list(split_data):      \n",
        "    list = [item for sublist in split_data for item in sublist]\n",
        "    return list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yarc5Hl6SfLh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words_train = getting_words_list(split_train_data)   #train data words\n",
        "words_test = getting_words_list(split_test_data)     #test data words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHhagAXTSfLl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "unique_words = len(set(words_train)) # getting number of unique words in train data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "LeKkNvwMSfLo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#geting the dictionary(model) from the train data, where keys are the history words and values are \n",
        "#another dictionary which contains keys as the next word and its values corresponding to the number \n",
        "#of times that words has occurred\n",
        "\n",
        "dic = {}\n",
        "for i in range(0,len(words_train)-1):\n",
        "    t = words_train[i]\n",
        "    s = dic.get(t)\n",
        "    next = i+1\n",
        "    if s == None:\n",
        "        dic[t] = [words_train[next]]\n",
        "    else:\n",
        "        s.append(words_train[next])\n",
        "        temp = {t:s}\n",
        "        dic.update(temp)\n",
        "#test_dic is a dictionary where keys are the history words of the test data and values is the list of next words     \n",
        "\n",
        "test_dic = {}\n",
        "for i in range(0,len(words_test)-1):\n",
        "    t = words_test[i]\n",
        "    s = test_dic.get(t)\n",
        "    next = i+1\n",
        "    if s == None:\n",
        "        test_dic[t] = [words_test[next]]\n",
        "    else:\n",
        "        s.append(words_test[next])\n",
        "        temp = {t:s}\n",
        "        test_dic.update(temp)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ry851apSfLr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# getting the count of each element in the list\n",
        "def frequency_count(list):\n",
        "    c = collections.Counter(list)\n",
        "    count_dic={}\n",
        "    for i in range(0,len(list)):\n",
        "        count_dic[list[i]] = c[list[i]]\n",
        "    return count_dic\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-S8_G2PqSfLv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# updating the dic with the word frequency\n",
        "for word in dic:\n",
        "    s = dic.get(word)\n",
        "    value = frequency_count(s)\n",
        "    temporary = {word:value}\n",
        "    dic.update(temporary)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beQ2ssOmSfLy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# getting the probability of each predicted word\n",
        "def get_probability(s1,s2):\n",
        "    firstWord = s1\n",
        "    nextWord = s2\n",
        "    inner_dic = dic.get(firstWord)\n",
        "    \n",
        "    if inner_dic == None:\n",
        "        return 0\n",
        "    else:\n",
        "        numerator = inner_dic.get(nextWord)\n",
        "        if numerator == None:\n",
        "            return 0\n",
        "        else:\n",
        "            denominator = sum(inner_dic.values())\n",
        "            return numerator/denominator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ur02L__XSfL0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# getting the add-one probability of each predicted word\n",
        "def get_addOne_probability(s1,s2):\n",
        "    firstWord = s1\n",
        "    nextWord = s2\n",
        "    inner_dic = dic.get(firstWord)\n",
        "    if inner_dic == None:\n",
        "        return 0 \n",
        "    else:\n",
        "        numerator = inner_dic.get(nextWord)\n",
        "        if numerator == None:\n",
        "            numerator = 1\n",
        "        else:\n",
        "            numerator = numerator + 1\n",
        "        denominator = sum(inner_dic.values()) + unique_words\n",
        "    return numerator/denominator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMUXXRtWSfL3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# getting the add-alpha probability of each predicted word\n",
        "def get_addAlpha_probability(s1,s2):\n",
        "    firstWord = s1\n",
        "    nextWord = s2\n",
        "    alpha = 0.1\n",
        "    inner_dic = dic.get(firstWord)\n",
        "    if inner_dic == None:\n",
        "        return 0 \n",
        "    else:\n",
        "        numerator = inner_dic.get(nextWord)\n",
        "        if numerator == None:\n",
        "            numerator = alpha\n",
        "        else:\n",
        "            numerator = numerator + alpha\n",
        "        denominator = sum(inner_dic.values()) + (unique_words*alpha)\n",
        "    return numerator/denominator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIZ-zC5GSfL7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#preparing probability dictionaries with corresponding probabilities\n",
        "prob_dic = {}\n",
        "addOneProb_dic = {}\n",
        "addAlphaProb_dic = {}\n",
        "for i in range(0,len(words_test)-2):\n",
        "    bigram = words_test[i]+words_test[i+1]\n",
        "    prob = get_probability(words_test[i],words_test[i+1])\n",
        "    addOneProb = get_addOne_probability(words_test[i],words_test[i+1])\n",
        "    prob_dic[bigram]=prob\n",
        "    addOneProb_dic[bigram] = addOneProb\n",
        "    addAlphaProb = get_addAlpha_probability(words_test[i],words_test[i+1])\n",
        "    addAlphaProb_dic[bigram] = addAlphaProb\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29rvwX-ySfL_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# computing perplexity Score\n",
        "def perplexityScore(prob_dic):\n",
        "    logProb = 0\n",
        "    num = 0\n",
        "    for word in prob_dic:\n",
        "        p = prob_dic.get(word)\n",
        "        if p == 0:\n",
        "            logProb = logProb\n",
        "        else:\n",
        "            w = m.log(p**-1,10)\n",
        "            logProb = logProb + w \n",
        "            num = num + 1\n",
        "    return (2**(logProb/num))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7tcT74ESfMB",
        "colab_type": "code",
        "colab": {},
        "outputId": "bc803d48-3699-420f-d198-2daa32556857"
      },
      "source": [
        "# perplexity Scores for different cases\n",
        "ans = perplexityScore(prob_dic)\n",
        "print(ans)\n",
        "ans1 = perplexityScore(addOneProb_dic)\n",
        "print(ans1)\n",
        "ans = perplexityScore(addAlphaProb_dic)\n",
        "print(ans1)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.1609533939954617\n",
            "14.267052487965701\n",
            "14.267052487965701\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80Pl6RrkSfME",
        "colab_type": "code",
        "colab": {},
        "outputId": "bd03beda-93fb-4d02-e13d-549a62fa6834"
      },
      "source": [
        "#input interface\n",
        "s1 = input(\"Enter the 1st word \" )\n",
        "input_word = s1\n",
        "print(input_word)\n",
        "def getting_2nd(input_word):\n",
        "    s = dic.get(input_word)\n",
        "    if s == None:\n",
        "        print(\"not found\")\n",
        "        return\n",
        "    else:\n",
        "        maximum = max(s, key=s.get)\n",
        "        return maximum\n",
        "print(getting_2nd(input_word))\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter the 1st word గుంటూరు\n",
            "గుంటూరు\n",
            "జిల్లా\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSI9B0obSfMG",
        "colab_type": "code",
        "colab": {},
        "outputId": "fa1dbbd8-9b28-4324-aec5-c46b88b70854"
      },
      "source": [
        "#predicting a sequence 9 of words\n",
        "s1 = input(\"Enter the 1st word \" )\n",
        "input_word = s1\n",
        "i = 1\n",
        "next = getting_2nd(input_word)\n",
        "for i in range(1,20):\n",
        "        print(next)\n",
        "        next2 = getting_2nd(next)\n",
        "        next = next2\n",
        "       "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter the 1st word గుంటూరు\n",
            "జిల్లా\n",
            "కమలాపురం\n",
            "తాలూకా\n",
            "ఎఱ్ఱగుడిపాడులో\n",
            "లభించినది\n",
            "</s>\n",
            "<s>\n",
            "ఈ\n",
            "రోజుల్లో\n",
            "కంప్యూటర్\n",
            "కి\n",
            "అర్థం\n",
            "కాదు\n",
            "</s>\n",
            "<s>\n",
            "ఈ\n",
            "రోజుల్లో\n",
            "కంప్యూటర్\n",
            "కి\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}